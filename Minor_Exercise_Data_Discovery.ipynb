{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-mitchell",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37641,
     "status": "ok",
     "timestamp": 1618169074626,
     "user": {
      "displayName": "Darragh Tate",
      "photoUrl": "",
      "userId": "15949939060990236508"
     },
     "user_tz": -60
    },
    "id": "premier-mitchell",
    "outputId": "ae5283e6-22c7-404e-8687-5a8105643aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from scikit-learn->sklearn) (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "# Darragh Tate\n",
    "\n",
    "# The diamond data set comprises of 53940 records, with each record containing the selling price, cut, colour, \n",
    "# clarity, depth percentage (the depth / width), table percentage (the flat polished top), height, width and depth.\n",
    "\n",
    "# Data Source: \"https://vincentarelbundock.github.io/Rdatasets/doc/ggplot2/diamonds.csv\"\n",
    "# Data Documentation: \"https://vincentarelbundock.github.io/Rdatasets/doc/ggplot2/diamonds.html\"\n",
    "\n",
    "\n",
    "# Data Collection and Cleaning\n",
    "\n",
    "! pip install pandas\n",
    "! pip install sklearn\n",
    "! pip install numpy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Used to set random states, for consistency\n",
    "random_state = 23\n",
    "\n",
    "\n",
    "df = pd.read_csv('diamonds.csv')\n",
    "\n",
    "df = df[[\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"]]\n",
    "# .astype is pandas categorisation function.\n",
    "df[['cut', 'color', 'clarity']] = df[['cut', 'color', 'clarity']].astype('category')\n",
    "# .cat.codes is pandas function to take the categorical information and replace the string variables with the numerics\n",
    "df['cut'] = df['cut'].cat.codes\n",
    "df['color'] = df['color'].cat.codes\n",
    "df['clarity'] = df['clarity'].cat.codes\n",
    "\n",
    "# x is the dataframe conaining the independant variables\n",
    "X_values = df[[\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"x\",\"y\",\"z\"]]\n",
    "# y is the dataframe containing the dependant variables\n",
    "y_values = df['price']\n",
    "\n",
    "# Legacy code - Used to split the price bands into categories.\n",
    "categories = pd.qcut(y_values, 10)\n",
    "y_categorical = pd.DataFrame(y_values, columns={'price'})\n",
    "y_categorical.loc[(y_categorical['price']<500), \"categories\"] = \"< 500\"\n",
    "y_categorical.loc[(y_categorical['price']>=500) & (y_categorical['price']<1000), 'categories'] = \"500 - 1000\"\n",
    "y_categorical.loc[(y_categorical['price']>=1000) & (y_categorical['price']<1500), 'categories'] = \"1000 - 1500\"\n",
    "y_categorical.loc[(y_categorical['price']>=1500) & (y_categorical['price']<2000), 'categories'] = \"1500 - 2000\"\n",
    "y_categorical.loc[(y_categorical['price']>=2000) & (y_categorical['price']<2500), 'categories'] = \"2000 - 2500\"\n",
    "y_categorical.loc[(y_categorical['price']>=2500) & (y_categorical['price']<3000), 'categories'] = \"2500 - 3000\"\n",
    "y_categorical.loc[(y_categorical['price']>=3000) & (y_categorical['price']<3500), 'categories'] = \"3000 - 3500\"\n",
    "y_categorical.loc[(y_categorical['price']>=3500), \"categories\"] = \"3500+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "existing-three",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5739,
     "status": "ok",
     "timestamp": 1618169123473,
     "user": {
      "displayName": "Darragh Tate",
      "photoUrl": "",
      "userId": "15949939060990236508"
     },
     "user_tz": -60
    },
    "id": "existing-three",
    "outputId": "d4562cbe-c4a8-48e7-cb16-d26ade0ef798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared (No polynomials): 0.88507\n",
      "\n",
      "R squared with polynomial degree of 4: 0.95211\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression / Polynomial Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Linear regression conceptually plots the data on a graph and tries to draw a line between the different data points.\n",
    "# By plotting additional data points on to the trained line, the algorithm can determine an an approximate value.\n",
    "# In simpler terms, if this graph had the dependant variable (say \"height\") on the y-axis and the independent variable (say \"age\")\n",
    "# on the x-axis, we could see the projected heights of someone of any age by seeing where the line lies along the y-axis in\n",
    "# relation to the age on the x-axis.\n",
    "# Polynomial regression allows for this line to bend, and as such is is much more useful.\n",
    "# It is very easy to overfit this model, however, as drawing a line straight through every point isn't generalised, it's tied to\n",
    "# the training data. \n",
    "# Also, as the algorithm takes every point into consideration, outlying variables can really throw off the line and the projected variables.\n",
    "\n",
    "linear_model = LinearRegression(copy_X = True, fit_intercept = True, n_jobs = None, normalize = False)\n",
    "\n",
    "# Fits the model with the training data\n",
    "linear_model.fit(X_values, y_values)\n",
    "\n",
    "# Prediction for the y-values by testing the X values with the algorithm.\n",
    "y_pred= linear_model.predict(X_values)\n",
    "\n",
    "# The correlation between the actual y-values and the ones predicted by the model\n",
    "score = r2_score(y_values, y_pred)\n",
    "print(f\"R squared (No polynomials): {score:.5f}\\n\")\n",
    "\n",
    "# This fits the model with a 4-th degree polynomial. This resulted in the highest r^2 score, however it may well be overfit.\n",
    "degree = 4\n",
    "poly_reg_model = PolynomialFeatures(degree = degree)\n",
    "\n",
    "# Transforms the X_values into their scaled version as defined by poly_reg_model\n",
    "X_poly = poly_reg_model.fit_transform(X_values)\n",
    "polynomial_model = LinearRegression()\n",
    "\n",
    "# Fits the new X_poly values to the new model\n",
    "polynomial_model.fit(X_poly, y_values)\n",
    "y_poly_prediction = polynomial_model.predict(X_poly)\n",
    "poly_score = r2_score(y_values, y_poly_prediction)\n",
    "\n",
    "print(f'R squared with polynomial degree of {degree}: {poly_score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "balanced-saver",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25400,
     "status": "ok",
     "timestamp": 1618169107084,
     "user": {
      "displayName": "Darragh Tate",
      "photoUrl": "",
      "userId": "15949939060990236508"
     },
     "user_tz": -60
    },
    "id": "balanced-saver",
    "outputId": "9853f281-365e-45cc-c6ee-72dccf9dcb0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score with 1 neighbours: 0.9245091644918888\n",
      "New best model found\n",
      "Model score with 2 neighbours: 0.9392135331132148\n",
      "New best model found\n",
      "Model score with 3 neighbours: 0.9426962202475522\n",
      "New best model found\n",
      "Model score with 4 neighbours: 0.9447424162405536\n",
      "New best model found\n",
      "Model score with 5 neighbours: 0.9451085392000287\n",
      "New best model found\n",
      "Model score with 6 neighbours: 0.9450532346230047\n",
      "Model score with 7 neighbours: 0.9448318982497693\n",
      "Model score with 8 neighbours: 0.9445590631890071\n",
      "Model score with 9 neighbours: 0.9439180670086385\n",
      "Model score with 10 neighbours: 0.9434160956921591\n",
      "Model score with 11 neighbours: 0.9426248259983606\n",
      "Model score with 12 neighbours: 0.9422821707821275\n",
      "Model score with 13 neighbours: 0.9419353371012229\n",
      "Model score with 14 neighbours: 0.9416948168679979\n",
      "Model score with 15 neighbours: 0.941127821554051\n",
      "Model score with 16 neighbours: 0.940497980151835\n",
      "Model score with 17 neighbours: 0.9398692445353455\n",
      "Model score with 18 neighbours: 0.9391613860948957\n",
      "Model score with 19 neighbours: 0.9386342652797823\n",
      "Model score with 20 neighbours: 0.9381091037604634\n",
      "Best Model found: 5 neighbours which gives an r^2 value of 0.9451085392000287\n"
     ]
    }
   ],
   "source": [
    "# K-Means Clustering\n",
    "# Score: 0.9451085392000287, with 5 neighbours\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# KNN is a clustering algorithm that uses distance to classify data points.\n",
    "# Essentially, the labelled data is plotted on a graph, and when the model is given unlabelled data, it is classified as belonging\n",
    "# to whatever class member is closer (as per Euclidean distance). The K value is used to determine how many points are used\n",
    "# for comparison. Foe example, if K = 1, then only the nearest labelled data point is used for comparison, but if K = 10, then\n",
    "# the 10 nearest points are used, and the unlabelled data is classified as whatever the majority of the K closest points are.\n",
    "# For example, if K = 10, 7 are class A and 3 are class B, then the data would be classified as being of class A.\n",
    "# It works better for categorical data, as that has more defined separation between classes, unlike continuous data, which\n",
    "# operates on a sliding scale and as such is harder to separate into clearly defined classes.\n",
    "# Knn Regressors work by finding the weighted averages of the K nearest neighbours, in this case of price.\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values, y_values, test_size = .33, random_state = random_state)\n",
    "\n",
    "# To save time in testing different numbers of neighbours, this iterates through it comparing differing values,\n",
    "# The best model is saved and displayed at the end.\n",
    "best_score = 0\n",
    "best_num_neighbours = 0\n",
    "for i in range(1, 21):\n",
    "    # K is i, the number the loop is iterating with\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=i)\n",
    "    # Fit the training data to the model\n",
    "    knn_model.fit(X_train, y_train.values.ravel())\n",
    "    model_score = knn_model.score(X_test, y_test)\n",
    "    print(f'Model score with {i} neighbours: {model_score}')\n",
    "    if (model_score > best_score):\n",
    "        print(f'New best model found')\n",
    "        best_score = model_score\n",
    "        best_num_neighbours = i\n",
    "\n",
    "print(f'Best Model found: {best_num_neighbours} neighbours which gives an r^2 value of {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-gazette",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140877,
     "status": "ok",
     "timestamp": 1617628405355,
     "user": {
      "displayName": "Darragh Tate",
      "photoUrl": "",
      "userId": "15949939060990236508"
     },
     "user_tz": -60
    },
    "id": "rocky-gazette",
    "outputId": "c8aaabfc-d5bc-4345-c2a0-7e7131a0d40c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.943772889411137\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "# Score: 0.943772889411137\n",
    "\n",
    "# Neural Networks are collections of Perceptrons, which are binary classifiers that can take in multiple sources of data \n",
    "# and \"activate\" (return a 1) if the activation function requirements are satisified by the input data.\n",
    "# The MLP models in scikit-learn are Multilayer Perceptrions, which facilitate non-binary outputs, allowing for more complex categorisation and regression.\n",
    "# Each node recieves data, and then is activated or not activated. This value is given a weight and passed on to the next node, and the process repeats.\n",
    "# After passing through multiple nodes, the final output can be either a classification or a calculated value (regression).\n",
    "# This uses the MLPRegressor, as we are predicting values, not splitting into classes.\n",
    "\n",
    "# Source: \"Hands-on Machine Learning with Scikit-Learn, Keras & Tensorflow\", Aurelien Geron, O'Reilly 2019, ISBN 978-1-492-03264-9, p279-289, Retrieved 10/04/'21\n",
    "\n",
    "# I believe the biggest issue with the Neural Network was model speed - using Colab with 12GB of RAM, it could take more than an\n",
    "# hour to get a model, which made testing hard (I had to use smaller sections of the dataset.)\n",
    "# It can also be very memory intensive - if the hidden layer sizes is too big, it can run out of memory (all 12 gb) and crash.\n",
    "# Lastly, it is a black box (can't inspect the decision making process), so if there is an error in the logic or dataset it's not easy to detect.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values, y_values, test_size = .33)\n",
    "\n",
    "# Scales the data to lie within the range of (x - u) / s, with x = sample, u = mean of x, s =standard deviation of x\n",
    "# Source: 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html', retrieved 12/04/'21\n",
    "# Compresses range so that outlying values don't have as drastic an effect on the algorithm\n",
    "scaling = StandardScaler().fit(X_train)\n",
    "\n",
    "# Fits the values to the scaling\n",
    "X_train = scaling.transform(X_train)\n",
    "X_test = scaling.transform(X_test)\n",
    "\n",
    "# Number of neurons in the hidden layers. This configuration was mostly decided by trial and error.\n",
    "hidden_layer_sizes = (10,20,40,80)\n",
    "\n",
    "# Maximum number of iterations available. Will stop early if results converge.\n",
    "max_iter = 10000\n",
    "\n",
    "# Rectified Linear Unit - activation function. Outputs 0 if negative input, or the input if positive.\n",
    "# Source: 'https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks', retrieved 12/04/'21\n",
    "activation = 'relu'\n",
    "\n",
    "# Adaptive Moment Estimation. Stochastic Optimisation as proposed by Diederik Kingma & Jimmy Ba.\n",
    "# Source: \"https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning\", retrieved 12/04/'21\n",
    "solver = 'adam'\n",
    "\n",
    "nn_reg = MLPRegressor(max_iter=max_iter, hidden_layer_sizes = hidden_layer_sizes, activation=activation, solver=solver, random_state=random_state)\n",
    "\n",
    "nn_reg.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = nn_reg.predict(X_test)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-movement",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142406,
     "status": "ok",
     "timestamp": 1618163457760,
     "user": {
      "displayName": "Darragh Tate",
      "photoUrl": "",
      "userId": "15949939060990236508"
     },
     "user_tz": -60
    },
    "id": "foreign-movement",
    "outputId": "fd281ac6-a068-405f-fe13-9583e4e9eae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9420355915848466\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine \n",
    "# Score: 0.9420355915848466\n",
    "\n",
    "# SVMs work by conceptually plotting the data on a scatteplot, then trying to draw lines that separate the data.\n",
    "# Data is analysed and placed on this conceptual plot\n",
    "# The idea is that margins are drawn along the plot which clearly separates the data, resulting in defined categorisation.\n",
    "# The support vector is the instance that lies along the margins of a class, i.e. the instance that is closest in definition to a memeber of another class. It is almost an outlier.\n",
    "# If data isn't linearly separable, then we must use soft margin classification (which allows outlying instances to be misclassified for the sake of model accuracy)\n",
    "# over hard margin classification (which only works if a straight line can be drawn between each class)\n",
    "\n",
    "# Source: \"Hands-on Machine Learning with Scikit-Learn, Keras & Tensorflow\", Aurelien Geron, O'Reilly 2019, ISBN 978-1-492-03264-9, p153-158, Retrieved 10/04/'21\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values, y_values, test_size = .33, random_state = random_state)\n",
    "\n",
    "# Scales the data to lie within the range of (x - u) / s, with x = sample, u = mean of x, s =standard deviation of x\n",
    "# Source: 'https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html', retrieved 12/04/'21\n",
    "# Compresses range so that outlying values don't have as drastic an effect on the algorithm\n",
    "scaling = StandardScaler().fit(X_train)\n",
    "X_train = scaling.transform(X_train)\n",
    "X_test = scaling.transform(X_test)\n",
    "\n",
    "#Maximum number of possible iterations. Will stop earl if data converges.\n",
    "max_iter = 500000\n",
    "\n",
    "# C represents tolerance, or width of the margins generated by the SVM. High values results in narrower margins, which can result in better training score at the risk of overfitting\n",
    "C = 1000 # 1000 & rbf = 0.9420355915848466\n",
    "#kernel = 'poly' # 0.8659278159023867\n",
    "#kernel = 'linear' # 0.8649820116013944\n",
    "\n",
    "# Radial Basis Function kernel. Honestly, I don't understand the maths behind this, asides it being linked to the squared euclidean distance between 2 data points.\n",
    "# It gave me the best score when training the model.\n",
    "kernel = 'rbf'\n",
    "\n",
    "# Degree is only used if kernel = 'poly', which was used druing development, so this is legacy code. \n",
    "# It's presence in the svm_model declaration makes no difference.\n",
    "degree = 1\n",
    "svm_model = SVR(max_iter = max_iter, C=C, kernel=kernel, degree = degree)\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(svm_model.score(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-europe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18850,
     "status": "ok",
     "timestamp": 1618157438053,
     "user": {
      "displayName": "Darragh Tate",
      "photoUrl": "",
      "userId": "15949939060990236508"
     },
     "user_tz": -60
    },
    "id": "literary-europe",
    "outputId": "69b2a034-3680-48c8-d496-7121feb3274e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9803378454248586\n"
     ]
    }
   ],
   "source": [
    "# Ensemble - Random Forest Regressor\n",
    "# Score: 0.9803378454248586\n",
    "\n",
    "# A random forest is an enhanced version of a decision tree.\n",
    "# It uses many decision trees on subsets of the data to classify them.\n",
    "# It uses averages of these trees to build a model, and as such it is able to deal with outlying data more easily, \n",
    "# as it will be averaged out due to regression to the mean.\n",
    "# The use of averages also minimises the risk of overfitting.\n",
    "\n",
    "# This provided the highest score that I was able to get for this dataset, and I believe that was due to the strengths\n",
    "# regarding the resistance to outlyers. As the diamonds dataset is about prices, and prices are set by people, it is\n",
    "# prone to bias (for example, certain diamond colours could appeal to certain people, resulting in a lack of price consistency)\n",
    "# Also, prices may change over time, and time isn't part of the dataset.\n",
    "# These factors may well result in some over or undervalued diamonds, which will be outlyers. The fact that random forests cancel\n",
    "# out the effects of outlyers by utilising means results in a more accurate and predictable model for less uniform datasets.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_values, y_values, test_size = .33, random_state = random_state)\n",
    "\n",
    "# Scales everything to lie between the values of 0 & 1. This reduces the damage caused by outliers on the model\n",
    "scaling = MinMaxScaler(feature_range=(0, 1)).fit(X_train)\n",
    "ensemble_model = RandomForestRegressor()\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "print(ensemble_model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1_Data_Discovery.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MLEnv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
