{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPaxJE3nVNZF",
    "outputId": "299357fa-cb78-499f-8d04-3261d3163a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from scikit-learn->sklearn) (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (1.20.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from nltk) (2021.3.17)\n",
      "Requirement already satisfied: joblib in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\darra\\desktop\\environments\\mlenv\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\darra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\darra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\darra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Collection & Cleaning\n",
    "\n",
    "! pip install sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "! pip install numpy\n",
    "import numpy as np\n",
    "! pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', random_state=17)\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return( [token.lower()for token in text if token not in stop_words] )\n",
    "\n",
    "def stem_words(text):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return([stemmer.lemmatize(token) for token in text])\n",
    "\n",
    "def stem_words_more(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return([stemmer.stem(token) for token in text])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuation = '!\"#$%&\\'()*+, -./:;<=>?@[\\]^_`{|}~'\n",
    "    return([char for char in text if char not in punctuation])\n",
    "\n",
    "def clean_data(input_list):\n",
    "    return_list = []\n",
    "    for li in input_list:\n",
    "        return_list.append(stem_words(remove_punctuation(remove_stop_words(tokenize(li)))))\n",
    "    return return_list\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "X, y = dataset.data, dataset.target\n",
    "data_size = len(X)\n",
    "X = X[:data_size]\n",
    "y = y[:data_size]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state = 23)\n",
    "X_train = clean_data(X_train)\n",
    "X_test = clean_data(X_test)\n",
    "y_train = y_train\n",
    "y_test = y_test\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy, preprocessor=dummy, token_pattern=None)\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSTVti8lVrwb",
    "outputId": "8ef4cc12-0cfe-4deb-b95d-680e0ef3f8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Score: 0.9241379310344827\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92       167\n",
      "           1       0.83      0.89      0.86       193\n",
      "           2       0.88      0.87      0.88       206\n",
      "           3       0.84      0.88      0.86       179\n",
      "           4       0.94      0.89      0.91       193\n",
      "           5       0.92      0.93      0.92       229\n",
      "           6       0.91      0.83      0.87       193\n",
      "           7       0.92      0.95      0.94       187\n",
      "           8       0.98      0.97      0.97       199\n",
      "           9       0.98      0.96      0.97       211\n",
      "          10       0.95      0.99      0.97       204\n",
      "          11       0.96      0.95      0.95       203\n",
      "          12       0.90      0.91      0.91       206\n",
      "          13       0.97      0.96      0.96       179\n",
      "          14       0.96      0.96      0.96       187\n",
      "          15       0.90      0.96      0.93       196\n",
      "          16       0.92      0.98      0.95       185\n",
      "          17       0.95      0.99      0.97       176\n",
      "          18       0.96      0.87      0.91       154\n",
      "          19       0.92      0.72      0.81       123\n",
      "\n",
      "    accuracy                           0.92      3770\n",
      "   macro avg       0.92      0.92      0.92      3770\n",
      "weighted avg       0.92      0.92      0.92      3770\n",
      "\n",
      "[[155   0   0   0   0   0   0   0   0   1   0   0   0   0   0   4   1   2\n",
      "    0   4]\n",
      " [  0 172   5   2   1   4   0   0   0   0   0   2   2   1   3   1   0   0\n",
      "    0   0]\n",
      " [  0   9 179   8   1   8   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   8 158   5   1   1   0   0   0   0   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   2   7 171   4   2   0   0   0   1   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   9   2   2   0 213   1   0   0   0   0   1   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   4   6   1   0 161   7   1   1   0   1   8   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1   0   0   4 178   2   0   0   0   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   4 193   0   0   0   1   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   0   0   0 203   6   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0 201   0   1   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   2   2   0   0   0   2   0   0   1   0 193   2   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   2   1   4   1   1   1   3   1   1   1   1 188   0   0   0   1   0\n",
      "    0   0]\n",
      " [  1   1   0   0   1   0   0   0   0   0   0   1   1 171   2   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   3   0   0   0   0   1   0   1 180   1   0   0\n",
      "    0   1]\n",
      " [  1   0   0   0   1   0   0   0   0   0   1   0   0   1   1 189   0   1\n",
      "    0   1]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0 181   0\n",
      "    3   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 175\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   0   1   0   1   1   0   0   0   1   1   7   5\n",
      "  134   2]\n",
      " [ 12   0   0   0   0   0   0   0   0   0   1   0   0   0   1  14   3   2\n",
      "    1  89]]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "# Naive Bayes Classification Score: 0.9241379310344827\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = MultinomialNB(alpha=0.01)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "print(f'Naive Bayes Classification Score: {nb_classifier.score(X_test, y_test)}')\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(f'Naive Bayes Classification Report:\\n{classification_report(y_test, y_pred)}\\n')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qgmtCnZmV1vv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Score: 0.8893899204244032\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89       167\n",
      "           1       0.67      0.90      0.77       193\n",
      "           2       0.81      0.86      0.83       206\n",
      "           3       0.88      0.87      0.88       179\n",
      "           4       0.97      0.87      0.91       193\n",
      "           5       0.95      0.91      0.93       229\n",
      "           6       0.59      0.88      0.71       193\n",
      "           7       0.93      0.89      0.91       187\n",
      "           8       0.98      0.89      0.93       199\n",
      "           9       0.99      0.93      0.96       211\n",
      "          10       0.95      0.94      0.95       204\n",
      "          11       1.00      0.90      0.95       203\n",
      "          12       0.87      0.84      0.86       206\n",
      "          13       0.93      0.94      0.94       179\n",
      "          14       0.96      0.93      0.94       187\n",
      "          15       0.90      0.89      0.90       196\n",
      "          16       0.95      0.94      0.95       185\n",
      "          17       0.98      0.95      0.96       176\n",
      "          18       0.97      0.83      0.90       154\n",
      "          19       0.88      0.67      0.76       123\n",
      "\n",
      "    accuracy                           0.89      3770\n",
      "   macro avg       0.90      0.88      0.89      3770\n",
      "weighted avg       0.90      0.89      0.89      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine Classification \n",
    "\n",
    "# SVM Classification Score: 0.8938992042440318\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm_classifier = SVC(kernel='poly', degree=2, max_iter = 5000)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "print(f'SVM Classification Score: {svm_classifier.score(X_test, y_test)}')\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(f'SVM Classification Report:\\n{classification_report(y_test, y_pred)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Qev9ZjFoWCS4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Classification Score: 0.920159151193634\n",
      "Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89       167\n",
      "           1       0.84      0.89      0.86       193\n",
      "           2       0.88      0.89      0.88       206\n",
      "           3       0.84      0.88      0.86       179\n",
      "           4       0.94      0.91      0.92       193\n",
      "           5       0.93      0.93      0.93       229\n",
      "           6       0.86      0.91      0.89       193\n",
      "           7       0.91      0.91      0.91       187\n",
      "           8       0.94      0.93      0.94       199\n",
      "           9       0.98      0.94      0.96       211\n",
      "          10       0.94      0.98      0.96       204\n",
      "          11       0.99      0.94      0.97       203\n",
      "          12       0.90      0.88      0.89       206\n",
      "          13       0.94      0.95      0.95       179\n",
      "          14       0.94      0.96      0.95       187\n",
      "          15       0.96      0.92      0.94       196\n",
      "          16       0.93      0.96      0.95       185\n",
      "          17       0.96      0.98      0.97       176\n",
      "          18       0.94      0.86      0.90       154\n",
      "          19       0.85      0.85      0.85       123\n",
      "\n",
      "    accuracy                           0.92      3770\n",
      "   macro avg       0.92      0.92      0.92      3770\n",
      "weighted avg       0.92      0.92      0.92      3770\n",
      "\n",
      "\n",
      "[[145   0   0   0   0   0   0   0   0   0   0   0   0   0   2   5   0   2\n",
      "    1  12]\n",
      " [  0 172  10   0   1   5   1   0   0   1   0   0   1   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   8 183   7   4   3   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   6 158   4   1   1   0   0   0   0   0   6   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   8 175   2   5   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   6   5   3   0 214   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   4   0   0 176   4   2   0   1   0   5   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   0   0   2   4 170   4   0   0   0   3   0   0   0   2   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   4   5 186   0   0   0   1   0   0   0   1   0\n",
      "    1   0]\n",
      " [  1   0   0   1   2   0   0   0   0 199   7   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   0   1   0   1 200   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  1   2   1   1   0   1   0   1   0   0   2 191   1   0   0   0   0   0\n",
      "    2   0]\n",
      " [  0   3   0   4   1   0   8   3   1   1   1   1 181   2   0   0   0   0\n",
      "    0   0]\n",
      " [  1   3   2   0   0   0   1   0   0   0   0   0   1 170   1   0   0   0\n",
      "    0   0]\n",
      " [  1   2   0   0   0   0   2   0   0   0   0   0   0   2 179   0   0   1\n",
      "    0   0]\n",
      " [  2   1   0   0   0   0   1   0   1   0   0   0   0   2   2 181   1   1\n",
      "    0   4]\n",
      " [  0   1   0   0   0   0   0   1   1   0   0   0   0   0   0   0 178   0\n",
      "    4   0]\n",
      " [  0   0   0   1   0   0   0   1   1   0   0   0   0   0   0   0   0 173\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   1   1   1   1   0   0   2   3   0   6   3\n",
      "  133   2]\n",
      " [  8   0   0   0   0   0   0   0   0   0   0   0   1   1   1   3   2   1\n",
      "    1 105]]\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Classification\n",
    "\n",
    "# Neural Network Classification Score: 0.9257294429708223\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "hidden_layer_sizes = 20\n",
    "max_iter = 50000\n",
    "solver = 'lbfgs' #'sgd' 'adam' 'lbfgs'\n",
    "activation = 'relu' # identity' #'relu'\n",
    "alpha = 0.1\n",
    "nn_classifier = MLPClassifier(solver=solver, activation=activation, hidden_layer_sizes = hidden_layer_sizes, alpha=alpha, max_iter=max_iter, verbose=False)\n",
    "nn_classifier.fit(X_train, y_train)\n",
    "print(f'Neural Network Classification Score: {nn_classifier.score(X_test, y_test)}')\n",
    "y_pred = nn_classifier.predict(X_test)\n",
    "print(f'Neural Network Classification Report:\\n{classification_report(y_test, y_pred)}\\n')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_2_Text_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
