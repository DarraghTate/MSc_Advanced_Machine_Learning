{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minor_Exercise_Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPaxJE3nVNZF",
        "outputId": "cfe3391e-04e5-4d01-892c-834c76fed871"
      },
      "source": [
        "# Name: Darragh Tate\n",
        "\n",
        "# Minor Exercise 2 - Text Classification\n",
        "# This assignment is about categorising different articles from the newsgroups dataset using 3 different ML techniques:\n",
        "#   - Naive Bayes Classification\n",
        "#   - Support Vector Machine Classification\n",
        "#   - Neural Network Classification\n",
        "\n",
        "# Data Collection & Cleaning\n",
        "\n",
        "! pip install sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "! pip install numpy\n",
        "import numpy as np\n",
        "! pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "dataset = fetch_20newsgroups(subset='all', random_state=17)\n",
        "\n",
        "# Returns the tokenized version of the input string, as determined by the punkt tokenizer\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Removes unimportant words from the sentence\n",
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return( [token.lower()for token in text if token not in stop_words] )\n",
        "\n",
        "# Returns the string with only root versions of words (e.g. \"loving\" changes to \"love\"). Reduces the number of words in the pool without losing context\n",
        "def stem_words(text):\n",
        "    stemmer = WordNetLemmatizer()\n",
        "    return([stemmer.lemmatize(token) for token in text])\n",
        "\n",
        "# More agressive version of stem_words, for example removes \"s\" from plural words (e.g \"bikes\" changes to \"bike\")\n",
        "def stem_words_more(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return([stemmer.stem(token) for token in text])\n",
        "\n",
        "# Gets rid of punctuation. String in function is every commonly used punctuation mark.\n",
        "def remove_punctuation(text):\n",
        "    punctuation = '!\"#$%&\\'()*+, -./:;<=>?@[\\]^_`{|}~'\n",
        "    return([char for char in text if char not in punctuation])\n",
        "\n",
        "# Calls all the above functions on each sentence in a given list\n",
        "def clean_data(input_list):\n",
        "    return_list = []\n",
        "    for li in input_list:\n",
        "        return_list.append(stem_words(remove_punctuation(remove_stop_words(tokenize(li)))))\n",
        "    return return_list\n",
        "\n",
        "def dummy(doc):\n",
        "    return doc\n",
        "\n",
        "X, y = dataset.data, dataset.target\n",
        "# Used to determine number of records, as the full data set is large and can have long processing times\n",
        "# Legacy code; Used for shorter test times during development of models\n",
        "data_size = len(X)\n",
        "X = X[:data_size]\n",
        "y = y[:data_size]\n",
        "\n",
        "# Splits the data into training & test data, random state is for consistency\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 23)\n",
        "X_train = clean_data(X_train)\n",
        "X_test = clean_data(X_test)\n",
        "\n",
        "# Converts data into tf-idf (Term Frequency over Item Document Frequency) Matrix. This tells us the relevance of each word in a document, by seeing how often it appears proportionally.\n",
        "tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy, preprocessor=dummy, token_pattern=None)\n",
        "\n",
        "#Fits the data to a matrix as defined with the tfidf object\n",
        "X_train = tfidf.fit_transform(X_train)\n",
        "X_test = tfidf.transform(X_test)"
      ],
      "execution_count": 1,
      "outputs": [
        
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSTVti8lVrwb",
        "outputId": "f42a1ae7-21a8-461f-8cce-316920fcfbb3"
      },
      "source": [
        "# Naive Bayes Classification\n",
        "\n",
        "# Naive Bayes Classification Score: 0.9198797311637779\n",
        "\n",
        "# Naive bayes is a probabilistic classifier that assumes (hence 'naive') independence between the factors, not assuming correlation between varaibles.\n",
        "# They collect average statistics on each class.\n",
        "# Data is compared to the averages of each class in the training set, and whichever is closest it is classified as.\n",
        "# Scikit-Learn has 3 classifiers: MultinomialNB (used for counts, such a word counts, making it ideal for this), BernouliiNB (for binary data) and GaussianNB (for continuous data)\n",
        "\n",
        "# Source - \"Introduction to Machine Learning with Python\", Andreas C. Muller & Sarah Guido, O'Reilly 2017, ISBN 978-1-449-36941-5, p 70-72, Retrieved 10/04/'21\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# alpha = model smoothness. Higher alpha results in a less complex model\n",
        "alpha = 0.01\n",
        "\n",
        "nb_classifier = MultinomialNB(alpha=alpha)\n",
        "\n",
        "# Fit the model with training data\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# .score calculates the r^2 values, or correlation, between the predicted and actual values\n",
        "print(f'Naive Bayes Classification Score: {nb_classifier.score(X_test, y_test)}')\n",
        "\n",
        "# Preditced classes of X_test, when fed through the trained model\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "print(f'Naive Bayes Classification Report:\\n{classification_report(y_test, y_pred)}\\n')\n",
        " \n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Classification Score: 0.9193491333569155\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.93      0.91       242\n",
            "           1       0.82      0.89      0.86       276\n",
            "           2       0.89      0.86      0.87       304\n",
            "           3       0.82      0.88      0.85       290\n",
            "           4       0.91      0.90      0.90       297\n",
            "           5       0.94      0.91      0.93       333\n",
            "           6       0.91      0.81      0.86       302\n",
            "           7       0.89      0.93      0.91       280\n",
            "           8       0.96      0.95      0.96       286\n",
            "           9       0.97      0.96      0.96       309\n",
            "          10       0.95      0.98      0.97       311\n",
            "          11       0.97      0.96      0.96       315\n",
            "          12       0.89      0.90      0.89       293\n",
            "          13       0.95      0.96      0.96       282\n",
            "          14       0.96      0.96      0.96       283\n",
            "          15       0.91      0.96      0.93       286\n",
            "          16       0.92      0.97      0.94       271\n",
            "          17       0.97      0.99      0.98       283\n",
            "          18       0.95      0.89      0.92       230\n",
            "          19       0.93      0.71      0.80       181\n",
            "\n",
            "    accuracy                           0.92      5654\n",
            "   macro avg       0.92      0.92      0.92      5654\n",
            "weighted avg       0.92      0.92      0.92      5654\n",
            "\n",
            "\n",
            "[[225   0   0   0   1   0   0   0   0   1   0   0   0   0   0   6   1   2   0   6]\n",
            " [  0 247   6   5   5   6   0   0   0   0   0   2   2   1   2   0   0   0   0   0]\n",
            " [  1  13 262  17   1   9   1   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   5   9 254  10   1   4   0   0   0   0   1   6   0   0   0   0   0   0   0]\n",
            " [  0   4   5   9 267   2   4   0   0   0   1   1   4   0   0   0   0   0   0   0]\n",
            " [  0  14   3   4   1 303   2   1   0   0   1   1   0   2   1   0   0   0   0   0]\n",
            " [  0   4   4  12   1   0 246  14   3   0   1   1  14   2   0   0   0   0   0   0]\n",
            " [  0   1   0   1   0   0   6 261   4   2   1   0   1   1   0   0   1   0   1   0]\n",
            " [  0   0   0   0   0   0   1   7 273   0   0   0   2   2   0   0   1   0   0   0]\n",
            " [  0   1   0   0   0   0   1   1   2 297   6   1   0   0   0   0   0   0   0   0]\n",
            " [  0   0   1   0   0   0   1   0   0   1 305   0   1   0   0   0   1   0   1   0]\n",
            " [  0   4   2   0   0   0   0   1   0   1   0 302   2   1   0   0   2   0   0   0]\n",
            " [  0   4   3   6   6   0   1   6   1   1   2   0 263   0   0   0   0   0   0   0]\n",
            " [  1   0   0   0   2   0   0   1   1   0   0   0   2 270   4   0   1   0   0   0]\n",
            " [  0   1   0   0   0   0   3   0   0   1   0   1   0   2 273   1   0   0   0   1]\n",
            " [  3   0   0   1   0   0   0   1   0   1   1   0   0   2   1 274   0   1   0   1]\n",
            " [  0   0   1   0   0   1   0   0   0   0   0   1   0   0   0   0 262   1   5   0]\n",
            " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 281   1   0]\n",
            " [  0   1   0   0   0   0   0   1   0   1   1   1   0   0   1   1  12   4 205   2]\n",
            " [ 21   0   0   0   0   0   0   0   0   1   1   0   0   0   1  20   4   2   3 128]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgmtCnZmV1vv",
        "outputId": "918002bb-0dc8-496b-c2d8-6eafed342dea"
      },
      "source": [
        "# Support Vector Machine Classification\n",
        "\n",
        "# SVC Classification Score: 0.9154580827732579\n",
        "\n",
        "# SVMs work by conceptually plotting the data on a scatteplot, then trying to draw lines that separate the data.\n",
        "# Data is analysed and placed on this conceptual plot\n",
        "# The idea is that margins are drawn along the plot which clearly separates the data, resulting in defined categorisation.\n",
        "# The support vector is the instance that lies along the margins of a class, i.e. the instance that is closest in definition to a memeber of another class. It is almost an outlier.\n",
        "# If data isn't linearly separable, then we must use soft margin classification (which allows outlying instances to be misclassified for the sake of model accuracy)\n",
        "# over hard margin classification (which only works if a straight line can be drawn between each class)\n",
        "\n",
        "# Source: \"Hands-on Machine Learning with Scikit-Learn, Keras & Tensorflow\", Aurelien Geron, O'Reilly 2019, ISBN 978-1-492-03264-9, p153-158, Retrieved 10/04/'21\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "# Polynomial kernel allows for linear classification, but with the line \"bending\". \n",
        "# Poly with a degree of 1 for some reason results in higher scores than linear\n",
        "kernel = 'poly' \n",
        "#kernel = 'rbf'# Score = 0.814\n",
        "#kernel = 'sigmoid'# Score = 0.829\n",
        "#kernel = 'linear' # Score = 0.838\n",
        "degree = 1\n",
        "# NOTE: I theorise a higher score could be attained with a large degree value, however the training process is too slow to test (even in colab, which I was using)\n",
        "\n",
        "# Maximum number of iterations. Not guaranteed to iterate this many times, if the data converges (no signifcant loss over x iterations) beforehand it terminates\n",
        "max_iter = 5000\n",
        "\n",
        "# C represents tolerance, or width of the margins generated by the SVM. High values results in narrower margins, which can result in better training score at the risk of overfitting\n",
        "C = 100\n",
        "\n",
        "svm_classifier = SVC(C = C, kernel=kernel, degree = degree, max_iter=max_iter)\n",
        "\n",
        "# Fit the training data to the model\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# .score calculates the r^2 values, or correlation, between the predicted and actual values\n",
        "print(f'SVM Classification Score: {svm_classifier.score(X_test, y_test)}')\n",
        "\n",
        "# Preditced classes of X_test, when fed through the trained model\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "print(f'SVM Classification Report:\\n{classification_report(y_test, y_pred)}\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Classification Score: 0.9154580827732579\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.91       242\n",
            "           1       0.76      0.88      0.82       276\n",
            "           2       0.86      0.86      0.86       304\n",
            "           3       0.82      0.84      0.83       290\n",
            "           4       0.92      0.89      0.91       297\n",
            "           5       0.92      0.93      0.93       333\n",
            "           6       0.88      0.90      0.89       302\n",
            "           7       0.94      0.91      0.92       280\n",
            "           8       0.98      0.96      0.97       286\n",
            "           9       0.97      0.94      0.96       309\n",
            "          10       0.97      0.97      0.97       311\n",
            "          11       0.99      0.94      0.96       315\n",
            "          12       0.84      0.91      0.87       293\n",
            "          13       0.92      0.94      0.93       282\n",
            "          14       0.98      0.95      0.97       283\n",
            "          15       0.93      0.92      0.92       286\n",
            "          16       0.93      0.94      0.94       271\n",
            "          17       0.99      0.98      0.98       283\n",
            "          18       0.92      0.91      0.92       230\n",
            "          19       0.86      0.79      0.82       181\n",
            "\n",
            "    accuracy                           0.92      5654\n",
            "   macro avg       0.92      0.91      0.91      5654\n",
            "weighted avg       0.92      0.92      0.92      5654\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qev9ZjFoWCS4",
        "outputId": "e9b88b83-b1c9-4e8d-a952-fb0ca7b1fb77"
      },
      "source": [
        "# Neural Network Classification\n",
        "\n",
        "# Neural Network Classification Score: 0.9257294429708223\n",
        "\n",
        "# Neural Networks are collections of Perceptrons, which are binary classifiers that can take in multiple sources of data \n",
        "# and \"activate\" (return a 1) if the activation function requirements are satisified by the input data.\n",
        "# The MLP models in scikit-learn are Multilayer Perceptrions, which facilitate non-binary outputs, allowing for more complex categorisation and regression.\n",
        "# Each node recieves data, and then is activated or not activated. This value is given a weight and passed on to the next node, and the process repeats.\n",
        "# After passing through multiple nodes, the final output can be either a classification or a calculated value (regression).\n",
        "# This uses the MLPClassifier, as we are splitting the text into classes.\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Artefact testing of different hidden layer structures\n",
        "#hidden_layer_sizes = (22,33)\n",
        "#hidden_layer_sizes = (22,33,)\n",
        "#hidden_layer_sizes = (25, 50,)# 0.8892819243013795\n",
        "#hidden_layer_sizes = (40, 80,) # 0.8997170145030067\n",
        "#hidden_layer_sizes = (100, 200,) # 0.90484612663600994\n",
        "#hidden_layer_sizes = (10,) # 0.8935267067562788\n",
        "#hidden_layer_sizes = (20,) # 0.8960028298549699\n",
        "#hidden_layer_sizes = (50,) # 0.8802617615847188\n",
        "#hidden_layer_sizes = (100,) # 0.9078528475415635\n",
        "#hidden_layer_sizes = (10, 20,) # 0.8608065086664308\n",
        "#hidden_layer_sizes = (110,) # Score: 0.911743898125221\n",
        "#hidden_layer_sizes = (120,) # 0.9094446409621507\n",
        "#hidden_layer_sizes = (115,) # Score: 0.8990095507605235\n",
        "hidden_layer_sizes = (110,)\n",
        "\n",
        "# Maximum number of iterations. Not guaranteed to iterate this many times, if the data converges (no signifcant loss over x iterations) beforehand it terminates\n",
        "max_iter = 50000\n",
        "\n",
        "# limited memory Broyden-Fletcher-Goldfarb-Shannon algorithm\n",
        "solver = 'lbfgs' #'sgd' 'adam' 'lbfgs'\n",
        "\n",
        "# Rectified  Linear Activation\n",
        "activation = 'relu' # identity' \n",
        "\n",
        "#alpha = model smoothness. Higher alpha results in a less complex model\n",
        "alpha = 0.01\n",
        "nn_classifier = MLPClassifier(solver=solver, activation=activation, hidden_layer_sizes=hidden_layer_sizes ,alpha=alpha, max_iter=max_iter, verbose=False)\n",
        "\n",
        "# Fit the training data to the model\n",
        "nn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# .score calculates the r^2 values, or correlation, between the predicted and actual values\n",
        "print(f'Neural Network Classification Score: {nn_classifier.score(X_test, y_test)}')\n",
        "\n",
        "# y_pred is what the classifier predicts the test values will be categorised as\n",
        "y_pred = nn_classifier.predict(X_test)\n",
        "print(f'Neural Network Classification Report:\\n{classification_report(y_test, y_pred)}\\n')\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network Classification Score: 0.911743898125221\n",
            "Neural Network Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89       242\n",
            "           1       0.85      0.86      0.85       276\n",
            "           2       0.85      0.87      0.86       304\n",
            "           3       0.79      0.85      0.82       290\n",
            "           4       0.90      0.91      0.90       297\n",
            "           5       0.91      0.92      0.91       333\n",
            "           6       0.91      0.90      0.90       302\n",
            "           7       0.92      0.91      0.92       280\n",
            "           8       0.97      0.95      0.96       286\n",
            "           9       0.93      0.97      0.95       309\n",
            "          10       0.95      0.98      0.97       311\n",
            "          11       0.96      0.96      0.96       315\n",
            "          12       0.88      0.86      0.87       293\n",
            "          13       0.95      0.94      0.94       282\n",
            "          14       0.94      0.96      0.95       283\n",
            "          15       0.92      0.91      0.91       286\n",
            "          16       0.96      0.92      0.94       271\n",
            "          17       0.97      0.98      0.97       283\n",
            "          18       0.92      0.89      0.90       230\n",
            "          19       0.84      0.75      0.80       181\n",
            "\n",
            "    accuracy                           0.91      5654\n",
            "   macro avg       0.91      0.91      0.91      5654\n",
            "weighted avg       0.91      0.91      0.91      5654\n",
            "\n",
            "\n",
            "[[216   0   0   0   0   0   0   0   0   0   0   0   0   1   4   5   0   1   2  13]\n",
            " [  1 236  15   3   1  10   0   0   0   1   0   0   2   3   2   0   0   1   0   1]\n",
            " [  0   7 263  18   3   9   0   0   0   1   1   0   0   1   1   0   0   0   0   0]\n",
            " [  0   5  12 246  13   1   5   1   0   0   0   0   7   0   0   0   0   0   0   0]\n",
            " [  0   4   2   9 270   2   6   1   1   0   0   0   2   0   0   0   0   0   0   0]\n",
            " [  0  14   6   5   0 305   0   0   0   0   1   0   0   2   0   0   0   0   0   0]\n",
            " [  0   2   0  12   1   0 271   4   2   0   1   0   7   0   2   0   0   0   0   0]\n",
            " [  0   2   2   3   0   2   6 255   1   1   1   2   4   0   0   0   0   0   1   0]\n",
            " [  0   0   0   0   1   1   1   7 272   3   0   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   1   0   0 300   6   0   0   0   0   1   0   0   1   0]\n",
            " [  0   0   0   1   0   0   0   0   1   2 306   0   0   0   0   0   0   1   0   0]\n",
            " [  0   1   2   1   0   2   1   0   0   0   0 301   4   0   0   0   2   0   1   0]\n",
            " [  0   3   3  10   8   1   3   4   1   2   1   4 252   1   0   0   0   0   0   0]\n",
            " [  1   2   1   0   1   0   1   1   0   3   0   0   3 264   4   0   0   0   0   1]\n",
            " [  1   0   1   0   0   0   0   1   1   0   1   0   3   1 272   1   0   0   0   1]\n",
            " [  6   1   0   0   0   0   0   0   0   3   2   1   0   1   1 261   0   3   1   6]\n",
            " [  1   0   1   1   0   0   1   2   1   1   0   3   1   1   0   1 249   1   7   0]\n",
            " [  0   0   0   1   0   1   0   1   0   0   0   0   0   0   1   1   0 276   2   0]\n",
            " [  1   0   0   2   2   0   0   0   0   3   1   1   1   2   2   0   7   1 204   3]\n",
            " [ 17   0   0   0   0   0   1   0   0   2   1   0   0   2   1  15   2   1  3 136]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
